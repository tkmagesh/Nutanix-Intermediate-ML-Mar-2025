 ```python
Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length)
```
âœ” **`Embedding(...)`** â†’ Converts word indices (integers) into dense vector representations.  
âœ” **`input_dim=vocab_size`** â†’ Number of unique words in the vocabulary.  
âœ” **`output_dim=64`** â†’ Size of the embedding vector for each word (each word is mapped to a 64-dimensional vector).  
âœ” **`input_length=max_length`** â†’ Specifies the fixed length of input sequences (for LSTMs, RNNs, etc.).  

---

## ðŸ”¹ **Example Usage**
```python
import tensorflow as tf
import numpy as np

# Define an embedding layer
embedding_layer = tf.keras.layers.Embedding(input_dim=10, output_dim=4, input_length=5)

# Sample input (word indices)
sample_input = np.array([[1, 2, 3, 4, 5]])

# Get embedding output
embedded_output = embedding_layer(sample_input)

print("Input:", sample_input)
print("Embedded Output:\n", embedded_output.numpy())
```

### ðŸ”¹ **Output**
```
Input: [[1 2 3 4 5]]

Embedded Output:
 [[[-0.02  0.3  -0.4   0.1 ]
   [ 0.1  -0.2   0.6  -0.3 ]
   [-0.5   0.2  -0.1   0.7 ]
   [ 0.3  -0.4   0.2   0.5 ]
   [ 0.4   0.1  -0.3  -0.2 ]]]
```
ðŸ”¹ Each word index is mapped to a **dense vector of size 4** (randomly initialized and learned during training).

---

## ðŸŽ¯ **Why Use Word Embeddings?**
âœ… **Captures word relationships** â†’ Words with similar meanings get similar vector representations.  
âœ… **Reduces dimensionality** â†’ Instead of a one-hot vector (huge size), each word gets a fixed-size vector.  
âœ… **Improves NLP model performance** â†’ Helps LSTMs/RNNs learn context better.  

### Recurrent Neural Networks (RNNs)  

#### 1ï¸âƒ£ What is an RNN?  
A **Recurrent Neural Network (RNN)** is a type of neural network that is designed for **sequential data**. Unlike traditional neural networks (like feedforward networks), RNNs have a built-in "memory," allowing them to **remember previous inputs** while processing new ones.  

#### 2ï¸âƒ£ Why Do We Need RNNs?  
Many real-world problems involve **sequences** where the order of data matters. Examples include:  
âœ… **Text data** (predicting the next word in a sentence)  
âœ… **Speech recognition** (processing spoken words over time)  
âœ… **Time series forecasting** (predicting stock prices based on past trends)  
âœ… **Video processing** (analyzing frames over time)  

A traditional neural network treats inputs independently, but RNNs understand the **context** by remembering past information.  

#### 3ï¸âƒ£ How Does an RNN Work?  
- Instead of processing an entire input at once, RNNs process **one step at a time** and pass information forward.  
- Each time the network sees a new input, it **updates its hidden state** based on both the new input and the previous hidden state.  
- This allows the network to capture dependencies in sequential data.  

#### ğŸ”„ RNN Structure  
An RNN has:  
1. **Input Layer** â€“ Receives one part of the sequence at a time.  
2. **Hidden Layer(s)** â€“ Maintains memory of past inputs using a loop structure.  
3. **Output Layer** â€“ Produces predictions based on hidden states.  

#### ğŸ”¢ Mathematical Representation  
Each step in an RNN follows this formula:  
$$
h_t = f(W_h h_{t-1} + W_x x_t + b)
$$
where:  
- $ h_t $ = hidden state at time $ t $  
- $ W_h $, $ W_x $ = weight matrices  
- $ x_t $ = input at time $ t $  
- $ b $ = bias  
- $ f $ = activation function (usually **tanh** or **ReLU**)  

The output is often computed as:  
$$
y_t = W_y h_t + b_y
$$  

#### 4ï¸âƒ£ Problems with RNNs  
ğŸš¨ **Vanishing Gradient Problem**: When training deep RNNs, gradients (used for learning) can become **very small**, making it hard for the network to learn long-term dependencies.  
âœ… Solution: **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** networks were developed to solve this problem!  

#### 5ï¸âƒ£ RNN Example in Python (Using TensorFlow)  
Hereâ€™s how to create a simple RNN using **Keras (TensorFlow)**:  
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Define an RNN model
model = Sequential([
    SimpleRNN(50, activation='tanh', input_shape=(10, 1)),  # 50 neurons, input sequence of 10 time steps
    Dense(1)  # Output layer
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Model summary
model.summary()
```

#### 6ï¸âƒ£ Summary  
âœ… RNNs are designed for **sequential data**.  
âœ… They **remember past information** and use it to predict future steps.  
âœ… They suffer from the **vanishing gradient problem**, which LSTMs and GRUs help fix.  
âœ… They are widely used in **NLP, speech recognition, and time series forecasting**.  

--------
### ğŸ”¹ **How LSTM Works**  

**Long Short-Term Memory (LSTM)** is a special type of **Recurrent Neural Network (RNN)** designed to handle **long-range dependencies** and prevent issues like the **vanishing gradient problem**. LSTMs achieve this using **gates** that control the flow of information.  

---

### LSTM Architecture
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
         â”‚  Forget Gate â”‚   ğŸ›‘ Decides what to discard  
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
                â†“  
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
         â”‚  Input Gate  â”‚   âœ… Decides what to store  
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
                â†“  
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
       â”‚  Cell State (Ct)â”‚   ğŸ“¦ Long-term memory  
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
                â†“  
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
         â”‚  Output Gate â”‚   ğŸ¯ Decides what to pass forward  
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
                â†“  
         ğŸš€ **New Hidden State (ht)**   


## ğŸ”· **1ï¸âƒ£ LSTM Cell Structure**
Each LSTM cell has three main components:  
1. **Cell state ($ C_t $)** â†’ Memory of the network, which flows through time.  
2. **Hidden state ($ h_t $)** â†’ Short-term memory, output at each step.  
3. **Gates (Forget, Input, Output)** â†’ Control what information is kept or discarded.

---

## ğŸ”· **2ï¸âƒ£ LSTM Step-by-Step Computation**
### âœ… **Step 1: Forget Gate**
ğŸ‘‰ **Decides what past information to discard**  

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

- $ f_t $ â†’ Forget gate output (0 = forget, 1 = keep)  
- $ W_f $, $ b_f $ â†’ Learnable weights & bias  
- $ h_{t-1} $ â†’ Previous hidden state  
- $ x_t $ â†’ Current input  
- $ \sigma $ â†’ Sigmoid activation (outputs values between 0 and 1)  

ğŸ”¹ If $ f_t $ is close to **1**, the memory is **kept**. If close to **0**, it is **forgotten**.

---

### âœ… **Step 2: Input Gate**
ğŸ‘‰ **Decides what new information to add to the memory**  

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

$$
\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

$$
C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
$$

- $ i_t $ â†’ Input gate output (how much new info to store)  
- $ \tilde{C}_t $ â†’ Candidate memory (potential update to cell state)  
- $ C_t $ â†’ Updated cell state  

ğŸ”¹ The **forget gate** controls old memory, and the **input gate** adds new information.

---

### âœ… **Step 3: Output Gate**
ğŸ‘‰ **Decides what part of the memory to output as the hidden state**  

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

$$
h_t = o_t \cdot \tanh(C_t)
$$

- $ o_t $ â†’ Output gate activation  
- $ h_t $ â†’ Updated hidden state (what is passed to the next time step)  

ğŸ”¹ The final hidden state $ h_t $ is used for predictions.

---

## ğŸ”· **3ï¸âƒ£ LSTM Step-by-Step Flow**
âœ” **Step 1: Forget Gate** â†’ Forget irrelevant past info  
âœ” **Step 2: Input Gate** â†’ Add relevant new info  
âœ” **Step 3: Update Cell State** â†’ Store long-term memory  
âœ” **Step 4: Output Gate** â†’ Output relevant info  

---

## ğŸ”· **4ï¸âƒ£ LSTM vs. Simple RNN**
| Feature | Simple RNN | LSTM |
|---------|-----------|------|
| Handles long sequences? | âŒ No | âœ… Yes |
| Vanishing gradient issue? | âœ… Yes | âŒ No |
| Memory mechanism? | âŒ No | âœ… Yes (Cell State) |
| Used in NLP, Speech, Time-Series? | âš ï¸ Limited | âœ… Preferred |

---

### âœ… **Conclusion**
ğŸ”¹ LSTMs are **powerful** because they **store important past information** while **discarding irrelevant details**.  
ğŸ”¹ They **prevent vanishing gradients**, making them great for **long text, speech, and time-series data**.  

